apiVersion: kubeflow.org/v2beta1
kind: MPIJob
metadata:
  name: torch-ddp-mnist
spec:
  slotsPerWorker: 2
  sshAuthMountPath: /home/mpiuser/.ssh
  runPolicy:
    cleanPodPolicy: Running
    ttlSecondsAfterFinished: 304800
  launcherCreationPolicy: WaitForWorkersReady
  mpiReplicaSpecs:
    Launcher:
      replicas: 1
      template:
        spec:
          restartPolicy: OnFailure
          containers:
            - name: launcher
              image: docker.io/benepstein8/torch-ddp-mpi:v1.1
              imagePullPolicy: Always
              env:
                # NCCL stability on Ethernet
                - name: NCCL_DEBUG
                  value: INFO
                - name: TORCH_NCCL_ASYNC_ERROR_HANDLING
                  value: "1"
                # Extra torch debug while iterating
                - name: TORCH_DISTRIBUTED_DEBUG
                  value: DETAIL
              command: ["/bin/bash","-lc"]
              args:
                - |
                  set -euo pipefail

                  # Derive a single shared rendezvous target from the first host in the MPI hostfile
                  export MASTER_HOST=$(head -n1 /etc/mpi/hostfile | awk '{print $1}')
                  export MASTER_ADDR=$(getent hosts "${MASTER_HOST}" | awk '{print $1}')
                  export MASTER_PORT=${MASTER_PORT:-29500}
                  # Derive WORLD_SIZE from hostfile
                  WORLD_SIZE=$(
                    awk '{
                      c=0
                      for(i=1;i<=NF;i++){
                        if($i ~ /^slots=/){ split($i,a,"="); c=a[2] }
                      }
                      if(c==0) c=1
                      total+=c
                    } END { print total }' /etc/mpi/hostfile
                  )
                  export WORLD_SIZE

                  echo "MASTER_HOST=${MASTER_HOST} MASTER_ADDR=${MASTER_ADDR} MASTER_PORT=${MASTER_PORT} WORLD_SIZE=${WORLD_SIZE}"

                  mpirun -np ${WORLD_SIZE} \
                    -bind-to none -map-by slot \
                    -mca btl ^openib \
                    -x MASTER_ADDR -x MASTER_PORT -x WORLD_SIZE \
                    -x NCCL_DEBUG -x TORCH_NCCL_ASYNC_ERROR_HANDLING \
                    -x TORCH_DISTRIBUTED_DEBUG \
                    -x LD_LIBRARY_PATH -x PATH \
                    -x OMPI_MCA_btl_vader_single_copy_mechanism=none \
                    bash -lc '
                      export RANK=${OMPI_COMM_WORLD_RANK}
                      export LOCAL_RANK=${OMPI_COMM_WORLD_LOCAL_RANK}
                      # Optional backend override via env, defaults to nccl in your script
                      python /workspace/torch_mnist_ddp.py
                    '
              securityContext:
                runAsUser: 10001
                runAsNonRoot: true
                allowPrivilegeEscalation: false
                capabilities:
                  drop: ["ALL"]
    Worker:
      replicas: 2
      template:
        spec:
          restartPolicy: OnFailure
          containers:
            - name: worker
              image: docker.io/benepstein8/torch-ddp-mpi:v1.1
              imagePullPolicy: Always
              resources:
                requests:
                  nvidia.com/gpu: 2
                limits:
                  nvidia.com/gpu: 2
              securityContext:
                runAsUser: 10001
                runAsNonRoot: true
                allowPrivilegeEscalation: false
                capabilities:
                  drop: ["ALL"]
              command: ["/bin/bash","-lc"]
              args:
                - |
                  set -euo pipefail
                  exec /usr/sbin/sshd -D -e -f /home/mpiuser/.sshd_config
